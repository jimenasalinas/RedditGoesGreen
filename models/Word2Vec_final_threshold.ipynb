{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMag4pBfkkJ/ArYQYQYdAzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimenasalinas/RedditGoesGreen/blob/main/models/Word2Vec_final_threshold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from datasets import Dataset\n",
        "import random\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "mD2RUX7XHXbH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/My Drive/group_project/archive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZP_3KFOHu8V",
        "outputId": "65c83b42-56c4-4aa5-e604-4171539809e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "AqqP9W_99pqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold data\n",
        "dtype_dict = {'label': int}\n",
        "comments = pd.read_csv(data_path + \"comments_filtered_by_threshold.csv\", quoting=csv.QUOTE_NONNUMERIC, dtype=dtype_dict)"
      ],
      "metadata": {
        "id": "1sAFcvdhX3Zy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and target label\n",
        "X = comments.drop('label', axis=1)  # Features (all columns except 'label')\n",
        "y = comments['label']  # Target label\n",
        "\n",
        "# Splitting the dataset into training and testing sets with 80% training data and 20% testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=120938)\n"
      ],
      "metadata": {
        "id": "IjDTN4zk-gAb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of observations for the smaller datasets\n",
        "train_obs = 15000\n",
        "test_obs = 1500\n",
        "\n",
        "# Training set\n",
        "# Shuffling DataFrame\n",
        "X_train_shuffled = X_train.sample(n=len(X_train), random_state=42).reset_index(drop=True)\n",
        "y_train_shuffled = y_train.sample(n=len(y_train), random_state=42).reset_index(drop=True)\n",
        "# Selecting the first 'train_obs' samples\n",
        "small_X_train = X_train_shuffled.head(train_obs)\n",
        "small_y_train = y_train_shuffled.head(train_obs)\n",
        "\n",
        "# Testing set\n",
        "# Shuffling DataFrame\n",
        "X_test_shuffled = X_test.sample(n=len(X_test), random_state=42).reset_index(drop=True)\n",
        "y_test_shuffled = y_test.sample(n=len(y_test), random_state=42).reset_index(drop=True)\n",
        "# Selecting the first 'test_obs' samples\n",
        "small_X_test = X_test_shuffled.head(test_obs)\n",
        "small_y_test = y_test_shuffled.head(test_obs)"
      ],
      "metadata": {
        "id": "fhBLSKVFAErY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(transcript):\n",
        "    words = transcript.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def clean_transcript(df):\n",
        "    df['clean_transcript'] = df['body'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    df['clean_transcript'] = df['clean_transcript'].astype(str).apply(remove_stopwords)\n",
        "    df.dropna(subset=['clean_transcript'], inplace=True)\n",
        "    return df\n",
        "\n",
        "# Apply cleaning and stopword removal to your dataset\n",
        "small_X_train = clean_transcript(small_X_train)\n",
        "small_X_test = clean_transcript(small_X_test)\n",
        "\n",
        "# Tokenize the cleaned transcripts\n",
        "small_X_train['tokenized'] = small_X_train['clean_transcript'].apply(lambda x: x.split())\n",
        "small_X_test['tokenized'] = small_X_test['clean_transcript'].apply(lambda x: x.split())"
      ],
      "metadata": {
        "id": "9lL9hoisCzbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=small_X_train['tokenized'], vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate a feature vector for each document\n",
        "def comment_vector(comment):\n",
        "    # remove out-of-vocabulary words\n",
        "    comment = [word for word in comment if word in model.wv.index_to_key]\n",
        "    if not comment:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(model.wv[comment], axis=0)\n",
        "\n",
        "small_X_train['doc_vector'] = small_X_train['tokenized'].apply(comment_vector)\n",
        "small_X_test['doc_vector'] = small_X_test['tokenized'].apply(comment_vector)"
      ],
      "metadata": {
        "id": "xgSOM78KK62Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c813ac-d05e-4bf3-95b2-a76ccba362af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e43a2ef12af5>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  small_X_train['doc_vector'] = small_X_train['tokenized'].apply(comment_vector)\n",
            "<ipython-input-9-e43a2ef12af5>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  small_X_test['doc_vector'] = small_X_test['tokenized'].apply(comment_vector)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectors = np.stack(small_X_train['doc_vector'].values)\n",
        "X_test_vectors = np.stack(small_X_test['doc_vector'].values)"
      ],
      "metadata": {
        "id": "v5kqu3fDIdTS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Fit the model\n",
        "lr_model.fit(X_train_vectors, small_y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "lr_predictions = lr_model.predict(X_test_vectors)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(small_y_test, lr_predictions))\n",
        "print(\"F1 score:\", f1_score(small_y_test, lr_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01SCRKGnFnpP",
        "outputId": "65e06b9e-eaf4-4bda-fffc-2a9dc9347e61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.6213333333333333\n",
            "F1 score: 0.5977337110481586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train_vectors, small_y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "rf_predictions = rf_model.predict(X_test_vectors)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(small_y_test, rf_predictions))\n",
        "print(\"Classification Report:\", classification_report(small_y_test, rf_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2V_jJhsJBnk",
        "outputId": "ed10b5f8-7087-4b23-f2d4-ad334450dd61"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.5946666666666667\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.70      0.64       767\n",
            "           1       0.61      0.48      0.54       733\n",
            "\n",
            "    accuracy                           0.59      1500\n",
            "   macro avg       0.60      0.59      0.59      1500\n",
            "weighted avg       0.60      0.59      0.59      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_vectors, small_y_train)\n",
        "svm_predictions = svm_model.predict(X_test_vectors)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(small_y_test, svm_predictions))\n",
        "print(\"Classification Report:\", classification_report(small_y_test, svm_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seTQuiiCJxFM",
        "outputId": "442ccc62-ff3d-4ad7-a848-9cb1702d1405"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.6286666666666667\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.68      0.65       767\n",
            "           1       0.63      0.57      0.60       733\n",
            "\n",
            "    accuracy                           0.63      1500\n",
            "   macro avg       0.63      0.63      0.63      1500\n",
            "weighted avg       0.63      0.63      0.63      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Defining the model\n",
        "model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(X_train_vectors.shape[1],)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_vectors, small_y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_vectors, small_y_test)\n",
        "print(\"Neural Network Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGK8U4FANyzd",
        "outputId": "435ccf26-6086-4af5-d5eb-5e5875681f02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 3ms/step - loss: 0.6794 - accuracy: 0.5695\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6693 - accuracy: 0.5865\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6665 - accuracy: 0.5925\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.6630 - accuracy: 0.6013\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.6059\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6573 - accuracy: 0.6075\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6563 - accuracy: 0.6075\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6575 - accuracy: 0.6075\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6532 - accuracy: 0.6109\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6524 - accuracy: 0.6108\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.5773\n",
            "Neural Network Accuracy: 0.5773333311080933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained model**"
      ],
      "metadata": {
        "id": "CBBJ_qTZaVSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the Google News Word2Vec model\n",
        "corpus_path = data_path + \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "model_pretrained = KeyedVectors.load_word2vec_format(corpus_path, binary=True)\n",
        "\n",
        "# Generate a feature vector for each document\n",
        "def comment_vector(comment):\n",
        "    # Remove out-of-vocabulary words\n",
        "    comment = [word for word in comment if word in model_pretrained.key_to_index]\n",
        "    if not comment:\n",
        "        return np.zeros(model_pretrained.vector_size)\n",
        "    # Use model_pretrained directly to get word vectors\n",
        "    return np.mean([model_pretrained[word] for word in comment], axis=0)\n",
        "\n",
        "small_X_train['doc_vector_pretrained'] = small_X_train['tokenized'].apply(comment_vector)\n",
        "small_X_test['doc_vector_pretrained'] = small_X_test['tokenized'].apply(comment_vector)\n",
        "\n",
        "X_train_vectors_pretrained = np.stack(small_X_train['doc_vector_pretrained'].values)\n",
        "X_test_vectors_pretrained = np.stack(small_X_test['doc_vector_pretrained'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXUnxptBZSYm",
        "outputId": "126954f9-fb4e-442a-d724-0502f41bb08c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e9be0deafdcf>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  small_X_train['doc_vector_pretrained'] = small_X_train['tokenized'].apply(comment_vector)\n",
            "<ipython-input-13-e9be0deafdcf>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  small_X_test['doc_vector_pretrained'] = small_X_test['tokenized'].apply(comment_vector)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Fit the model\n",
        "lr_model.fit(X_train_vectors_pretrained, small_y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "lr_predictions = lr_model.predict(X_test_vectors_pretrained)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(small_y_test, lr_predictions))\n",
        "print(\"F1 Score:\", f1_score(small_y_test, lr_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqC-RkZLbMtM",
        "outputId": "fc6b6b29-954e-4197-bb17-7c65369ffb8e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.7273333333333334\n",
            "F1 Score: 0.7181254307374225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train_vectors_pretrained, small_y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "rf_predictions = rf_model.predict(X_test_vectors_pretrained)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(small_y_test, rf_predictions))\n",
        "print(\"Classification Report:\", classification_report(small_y_test, rf_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HjbOcacb7Vv",
        "outputId": "01a3853d-c5ef-4f8b-92a8-5e6d5db0f128"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.666\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.76      0.70       767\n",
            "           1       0.69      0.57      0.62       733\n",
            "\n",
            "    accuracy                           0.67      1500\n",
            "   macro avg       0.67      0.66      0.66      1500\n",
            "weighted avg       0.67      0.67      0.66      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(X_train_vectors_pretrained.shape[1],)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_vectors_pretrained, small_y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_vectors_pretrained, small_y_test)\n",
        "print(\"Neural Network Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azFzLKhScEs1",
        "outputId": "786f1d09-a8a8-4685-b3e1-da241ca62c35"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 3s 4ms/step - loss: 0.5537 - accuracy: 0.7160\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5132 - accuracy: 0.7482\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.5003 - accuracy: 0.7544\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4914 - accuracy: 0.7593\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4828 - accuracy: 0.7631\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4734 - accuracy: 0.7700\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4618 - accuracy: 0.7748\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4506 - accuracy: 0.7833\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4336 - accuracy: 0.7943\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4117 - accuracy: 0.8058\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.6667\n",
            "Neural Network Accuracy: 0.6666666865348816\n"
          ]
        }
      ]
    }
  ]
}